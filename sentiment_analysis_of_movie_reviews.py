# -*- coding: utf-8 -*-
"""Sentiment Analysis of Movie Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f8knggRZUcYUWlvDPVWohdXKait_lyCs
"""

# Step 1: Install & Import Required Libraries
!pip install kagglehub

import os
import pandas as pd
import numpy as np
import re
import string

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, f1_score, classification_report

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# Step 2: Download IMDb Dataset from KaggleHub

import kagglehub

# Download latest version of dataset
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")

print("Path to dataset files:", path)

# Step 3: Load Dataset

csv_file = "IMDB Dataset.csv"
df = pd.read_csv(os.path.join(path, csv_file))

print("Dataset Shape:", df.shape)
print(df.head())

# Step 4: Encode Sentiment Labels (positive=1, negative=0)

df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})
print(df['sentiment'].value_counts())

# Step 5: Text Preprocessing

def clean_text(text):
    text = re.sub(r"<.*?>", " ", text)  # Remove HTML tags
    text = text.translate(str.maketrans("", "", string.punctuation))  # Remove punctuation
    text = text.lower()  # Lowercase
    text = " ".join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

df['clean_review'] = df['review'].apply(clean_text)

print(df[['review','clean_review']].head())

# Step 6: Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(
    df['clean_review'], df['sentiment'], test_size=0.2, random_state=42
)

print("Training samples:", len(X_train))
print("Testing samples:", len(X_test))

# Step 7: Feature Extraction with TF-IDF

vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print("TF-IDF Shape (Train):", X_train_tfidf.shape)
print("TF-IDF Shape (Test):", X_test_tfidf.shape)

# Step 8: Train Models

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "SVM": LinearSVC()
}

results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[name] = {"Accuracy": acc, "F1-score": f1}

    print(f"{name} - Accuracy: {acc:.4f}, F1-score: {f1:.4f}")

# Step 9: Evaluate Models

print("\n=== Model Performance Comparison ===")
for name, metrics in results.items():
    print(f"{name}: Accuracy={metrics['Accuracy']:.4f}, F1={metrics['F1-score']:.4f}")

# Detailed classification report for Logistic Regression
print("\nClassification Report (Logistic Regression):")
print(classification_report(y_test, models["Logistic Regression"].predict(X_test_tfidf)))

# Step 10: Confusion Matrix Visualization

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Negative", "Positive"],
                yticklabels=["Negative", "Positive"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(title)
    plt.show()

# Plot for each model
for name, model in models.items():
    y_pred = model.predict(X_test_tfidf)
    plot_confusion_matrix(y_test, y_pred, f"Confusion Matrix - {name}")